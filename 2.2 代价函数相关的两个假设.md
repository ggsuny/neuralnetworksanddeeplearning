
## 代价函数相关的两个假设

反向传播算法的目标是计算代价函数相对于任何一个权重和偏移量的的偏导数(∂C/∂w) 和 (∂C/∂b)。为了解释反向传播算法是如何工作的，我们需要做两个假设。我们举一个代价函数的作为例子。上一章在等式6中，我们提到过这个平方代价函数：

$$$$C=\frac{1}{2n}\sum_{x}^{ }\left \| y(x)-a^L(x) \right \|^2 \qquad\qquad\qquad (26)$$$$ 

n是训练集的数量，sum是对所有的训练集x求和，y=y(x)是期望的输出。 L代表神经网络的层级。 $$a^L$$ = $$a^L(x)$$是当网络的输入是x时，对应的激活输出。
  所以，为了执行反向传播算法，我应该对代价函数C做什么假设呢？第一个假设为代价函数可以写作每一个训练数据x对应代价函数求和的平均值：$$C=\frac{1}{n}\sum_{x}{ }C_x$$.这样，对于单个训练样本的代价函数为$$C_x = \frac{1}{2}\left \| y-a_L \right \|^2 $$。这个假设对于本书中我们提到的其它所有代价函数都成立。

这么做假设的原因是，反向传播算法事实上是对每一个训练样本求偏导数$$\partial C_x / \partial w $$ 以及$$\partial Cx / \partial b$$。然后，$$\partial C / \partial w$$ 以及 $$\partial C / \partial b$$实际是所有训练样本计算之后的平均值。基于这些假设，假设训练样本x是确定的，去掉下标x，就可以把$$C_x$$写作C。我们最终还是引入了x了，不过把它当做一个恼人的标记好了。

我们对代价函数所做的第二个假设也可以通过神经网络的输出函数的形式表达：
![](http://neuralnetworksanddeeplearning.com/images/tikz18.png)

例如，二次代价函数符合这个要求，这样，单个训练样本的二次代价函数就可以写作：

$$C = \frac {1}{2} \left \| y - a_L  \right |\ ^2 = \frac{1}{2} \sum_{j}{ }(y_j - a_j^L)^2, \qquad \qquad \qquad (27) $$
这是个关于输出激活值的函数。当然，这个代价函数也依赖于输出y，或许你会琢磨，为什么不同时把它当做y的函数呢？这是因为训练样本x是固定的，输出值y也是固定的参数。对偏移量和权重做任何改变都不会改变它们。它们不来自于神经网络学习的结果。所以，仅仅把上述函数看做$$a_L$$的函数，而把y当做是函数中的一个参数就可以了。