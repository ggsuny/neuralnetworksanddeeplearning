
"plugins": [
    "mathjax"
]

# 第二章：反向传播
---

上一章，我们看到了神经网络是如何通过梯队下降算法学习权重和偏移量。不过，我们跳过了如何计算成本函数导数这部分。这确实是一块不小的缺失。本章我将介绍一个计算此类导数的快速算法，名字叫反向传播。

反向传播算法最早在1970年代就引入了，但直至1980年代，那篇由 David Rumelhart, Geoffrey Hinton, and Ronald Williams发表的著名论文之后，才名声鹊起。这篇论文讲述了几种神经网络，用到的反向传播算法要比之前所有的方法在学习上都快很多。这样就使得通过神经网络解决许多之前无解的问题成为可能。如今，反向传播是神经网络学习中的顶梁柱。
  相对于本书其它章节，这一章将会涉及到较多的数学知识。如果你对数学不感冒，你可能想跳过这一章节。毕竟，把反向传播当做一个黑盒，这样它的细节你完全可以不用关心。为什么要花时间学习呢？
  原因当然是为了更深入的理解。反向传播算法的核心是一个偏导表达式∂C/∂w，它是代价函数C对所有的权重w(或者偏移量b)进行求导。这个等式告诉我们，在改变w或b的时候，代价函数的变化有多快。表达式虽然有些复杂，也有它美丽的一面，每个元素都有自然和直观的解释。所以，反向传播算法不仅仅是一个快速的学习方法。它也可以帮助我们了解，如何通过改变权重和偏移量来影响网络整体行为这个过程的细节。所以还是值得学习一下他的细节。
 虽然如此，如果你想跳过这个章节，直接开始下一章节，也没问题。即使你把反向传播算法当做一个黑盒来理解，你在学习本书剩余部分的时候也没有任何问题。这本书稍后的部分可能会提到本章的一些结果。不过，对你应该没有障碍。

  ## 热身：用一个快速的基于矩阵的算法，来计算神经网络的输出
---
探讨反向传播算法之前，我们先热热身，先介绍一个快速的基于矩阵的方法，用它来计算神经网络的输出。实际上在上一张快结束的时候，我们已经看到这个算法的大概了，现在我们了解一下它的细节。这对于习惯反向传播算法中的各种符号是一个不错的方法。
  我们从一个比较难的符号开始，$w_{jk}^l他代表了第(l-1)层的第K个的神经元到第l层的第j个神经元之间连接的权重。例如，下图展示的是第二层中第四个神经元到第三层第三个神经元之间连接的权重。

![](http://neuralnetworksanddeeplearning.com/images/tikz16.png)

一开始，看起来有些难懂，确实需要花些时间来掌握它。不过，花些功夫你就会发现它其实很简单和直观了。可能你会认为，用j来描述输入神经元用k描述输出神经元会更加直观一些。稍后我会解释为什么会是这样。

 在标记网络中的偏移量和激活值时，我们会使用相同的逻辑。$b_j^l$表示第l层的第j个神经元的偏移量，$a_j^l$表示第l层第j个神经元的激活值。下图是一个例子：

![](http://neuralnetworksanddeeplearning.com/images/tikz17.png)

基于这些标记，第l层第j个神经元的激活值$a_j^l$和第l-1层神经元的激活值有关系，等式如下：

$$a_j^l = \ (23) $$ 

这里对视对l-1层的所有k个神经元求和。为了使用向量重写这个等式，我们把$w_l$定义为第l层的权重矩阵。权重向量wl的单个实体是连接l层所有神经元的权重。也就是说第j行第k列的权重是wjkl。类似，对于每一层的偏移量b，我们定义向量bl。你可以已经猜测出来了，偏移量向量的每一个实体为bjl，它代表的是第l层的每一个偏移量。最后，我们定义激活值向量al，它的每一个元素为ajl。

我们要重写等式23的最后一个元素是向量化其中的函数，例如σ。上一章我们简单提到过，这里回顾一下。想法是对向量中每一个元素执行σ函数，使用σ(v)来标记。也就是，σ(v)的每一个组成部分σ(v)j=σ(vj)。举个例子，如果函数f(x)=x^2。 向量形式的f函数效果如下：

xxxx  （等式24）

向量化的f函数会对向量中的每一个元素执行平方计算。

有了这些标记，等式(23)可以被重写为如下等式：
al=σ(wlal-1+bl)
$$a_l = σ(w^la^{l-1} + b_l) (25)$$

这个等式可以让我们从整体上看到第l层的激活值al是如何从第l-1层的激活值$a^{l-1}$中计算得来的：我们把l-1层的激活值乘以对应的权重，然后加上对应的第l层的偏移量。最后在计算σ函数。整体视角经常要比单个神经元视角看起来更简洁和容易。它可以帮助我们避免去理解可恶的角标，同时对算法保持精确的理解。向量化的表达式在实际操作中也非常有用，因为大多数矩阵库都提供了快速的矩阵的乘法加法和向量化。实际上，上一章的代码中，在计算网络行为的时候，我们已经悄悄使用了这个表达式。

使用等式25计算$a_l$的时候，我们顺便计算了中间值$z_l$ ≡ $w_la_{l-1}+b_l$。这个等式很有用，还是值得命名一下的：我们称$z_l$为l层神经元的加权输入。本章中，我们会多次使用这个加权输入。等式25有时会以加权输入的形式书写，$a_l$ = σ($z_l$)。 值得提到的一点是$z_l$的元素模式为：$z_j^l$ = xxxxx。 也就是说$z_l$的元素$z_j^l$它是l层第j个神经元激活函数的加权输入.


## 代价函数相关的两个假设

反向传播算法的目标是计算代价函数相对于任何一个权重和偏移量的的偏导数(∂C/∂w) 和 (∂C/∂b)。为了解释反向传播算法是如何工作的，我们需要做两个假设。我们举一个代价函数的作为例子。上一章在等式6中，我们提到过这个平方代价函数：
xxxx （26）[等式26]

n是训练集的数量，sum是对所有的训练集x求和，y=y(x)是期望的输出。 L代表神经网络的层级。 $a^L$ = $a^L(x)$是当网络的输入是x时，对应的激活输出。






