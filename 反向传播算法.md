
"plugins": [
    "katex"
]

# 第二章：反向传播
---

上一章，我们看到了神经网络是如何通过梯队下降算法学习权重和偏移量。不过，我们跳过了如何计算成本函数导数这部分。这确实是一块不小的缺失。本章我将介绍一个计算此类导数的快速算法，名字叫反向传播。

反向传播算法最早在1970年代就引入了，但直至1980年代，那篇由 David Rumelhart, Geoffrey Hinton, and Ronald Williams发表的著名论文之后，才名声鹊起。这篇论文讲述了几种神经网络，用到的反向传播算法要比之前所有的方法在学习上都快很多。这样就使得通过神经网络解决许多之前无解的问题成为可能。如今，反向传播是神经网络学习中的顶梁柱。
  相对于本书其它章节，这一章将会涉及到较多的数学知识。如果你对数学不感冒，你可能想跳过这一章节。毕竟，把反向传播当做一个黑盒，这样它的细节你完全可以不用关心。为什么要花时间学习呢？
  原因当然是为了更深入的理解。反向传播算法的核心是一个偏导表达式∂C/∂w，它是代价函数C对所有的权重w(或者偏移量b)进行求导。这个等式告诉我们，在改变w或b的时候，代价函数的变化有多快。表达式虽然有些复杂，也有它美丽的一面，每个元素都有自然和直观的解释。所以，反向传播算法不仅仅是一个快速的学习方法。它也可以帮助我们了解，如何通过改变权重和偏移量来影响网络整体行为这个过程的细节。所以还是值得学习一下他的细节。
 虽然如此，如果你想跳过这个章节，直接开始下一章节，也没问题。即使你把反向传播算法当做一个黑盒来理解，你在学习本书剩余部分的时候也没有任何问题。这本书稍后的部分可能会提到本章的一些结果。不过，对你应该没有障碍。

  ## 热身：用一个快速的基于矩阵的算法，来计算神经网络的输出
---
探讨反向传播算法之前，我们先热热身，先介绍一个快速的基于矩阵的方法，用它来计算神经网络的输出。实际上在上一张快结束的时候，我们已经看到这个算法的大概了，现在我们了解一下它的细节。这对于习惯反向传播算法中的各种符号是一个不错的方法。
  我们从一个比较难的符号开始，$$w_{jk}^l$$他代表了第(l-1)层的第k个的神经元到第l层的第j个神经元之间连接的权重。例如，下图展示的是第二层中第四个神经元到第三层第三个神经元之间连接的权重。

![](http://neuralnetworksanddeeplearning.com/images/tikz16.png)

一开始，看起来有些难懂，确实需要花些时间来掌握它。不过，花些功夫你就会发现它其实很简单和直观了。可能你会认为，用j来描述输入神经元用k描述输出神经元会更加直观一些。稍后我会解释为什么会是这样。

 在标记网络中的偏移量和激活值时，我们会使用相同的逻辑。$b_j^l$表示第l层的第j个神经元的偏移量，$$a_j^l$$表示第l层第j个神经元的激活值。下图是一个例子：

![](http://neuralnetworksanddeeplearning.com/images/tikz17.png)

基于这些标记，第l层第j个神经元的激活值$a_j^l$和第l-1层神经元的激活值有关系，等式如下：

$$
a_j^l = \sigma \left ( \sum_{k}^{ } w_{jk}^{l} a_k^{l-1} + b_j^l \right ), \qquad \qquad \qquad (23) 
$$ 

这里对视对l-1层的所有k个神经元求和。为了使用向量重写这个等式，我们把$$w_l$$定义为第l层的权重矩阵。权重向量wl的单个实体是连接l层所有神经元的权重。也就是说第j行第k列的权重是$$w_{jk}^l$$。类似，对于每一层的偏移量b，我们定义向量$$b_l$$。你可以已经猜测出来了，偏移量向量的每一个实体为$$b_j^l$$，它代表的是第l层的每一个偏移量。最后，我们定义激活值向量$$a_l$$，它的每一个元素为$$a_j^l$$。

我们要重写等式23的最后一个元素是向量化其中的函数，例如σ。上一章我们简单提到过，这里回顾一下。想法是对向量中每一个元素执行$$\sigma$$函数，使用$$\sigma (v) $$来标记。也就是，$$\sigma (v) $$的每一个组成部分$$\sigma (v)_j = \sigma (v_j)$$。举个例子，如果函数$$f(x)=x^2$$。 向量形式的f函数效果如下：
$$ 
f\begin{pmatrix}
\begin{bmatrix}
 2 \\ 
 3  
\end{bmatrix}
\end{pmatrix} 

= 

\begin{bmatrix}
f(2)\\ 
f(3)
\end{bmatrix}

=

\begin{bmatrix}
4\\ 
9
\end{bmatrix}, \qquad\qquad\qquad (24)
$$


向量化的f函数会对向量中的每一个元素执行平方计算。

有了这些标记，等式(23)可以被重写为如下等式：

$$a_l = \sigma (w^la^{l-1} + b_l) \qquad\qquad\qquad (25)$$

这个等式可以让我们从整体上看到第l层的激活值$$a_l$$是如何从第l-1层的激活值$$a^{l-1}$$中计算得来的：我们把l-1层的激活值乘以对应的权重，然后加上对应的第l层的偏移量。最后在计算σ函数。整体视角经常要比单个神经元视角看起来更简洁和容易。它可以帮助我们避免去理解可恶的角标，同时对算法保持精确的理解。向量化的表达式在实际操作中也非常有用，因为大多数矩阵库都提供了快速的矩阵的乘法加法和向量化。实际上，上一章的代码中，在计算网络行为的时候，我们已经悄悄使用了这个表达式。

使用等式25计算$$a^l$$的时候，我们顺便计算了中间值$$z^l$$ ≡ $$w^la^{l-1}+b^l$$。这个等式很有用，还是值得命名一下的：我们称$$z^l$$为l层神经元的加权输入。本章中，我们会多次使用这个加权输入。等式25有时会以加权输入的形式书写，$$a^l = \sigma (z^l) $$。 值得提到的一点是$$z^l$$的元素模式为：$$a_j^l = \sigma \left ( \sum_{k}^{ } w_{jk}^{l} a_k^{l-1} + b_j^l \right )$$ 。 也就是说$$z^l$$的元素$$z_j^l$$它是l层第j个神经元激活函数的加权输入.


## 代价函数相关的两个假设

反向传播算法的目标是计算代价函数相对于任何一个权重和偏移量的的偏导数(∂C/∂w) 和 (∂C/∂b)。为了解释反向传播算法是如何工作的，我们需要做两个假设。我们举一个代价函数的作为例子。上一章在等式6中，我们提到过这个平方代价函数：

$$C=\frac{1}{2n}\sum_{x}^{ }\left \| y(x)-a^L(x) \right \|^2 \qquad\qquad\qquad (26)$$ 

n是训练集的数量，sum是对所有的训练集x求和，y=y(x)是期望的输出。 L代表神经网络的层级。 $$a^L$$ = $$a^L(x)$$是当网络的输入是x时，对应的激活输出。
  所以，为了执行反向传播算法，我应该对代价函数C做什么假设呢？第一个假设为代价函数可以写作每一个训练数据x对应代价函数求和的平均值：$$C=\frac{1}{n}\sum_{x}{ }C_x$$.这样，对于单个训练样本的代价函数为$$C_x = \frac{1}{2}\left \| y-a_L \right \|^2 $$。这个假设对于本书中我们提到的其它所有代价函数都成立。

这么做假设的原因是，反向传播算法事实上是对每一个训练样本求偏导数$$\partial C_x / \partial w $$ 以及$$\partial Cx / \partial b$$。然后，$$\partial C / \partial w$$ 以及 $$\partial C / \partial b$$实际是所有训练样本计算之后的平均值。基于这些假设，假设训练样本x是确定的，去掉下标x，就可以把$$C_x$$写作C。我们最终还是引入了x了，不过把它当做一个恼人的标记好了。

我们对代价函数所做的第二个假设也可以通过神经网络的输出函数的形式表达：
![](http://neuralnetworksanddeeplearning.com/images/tikz18.png)

例如，二次代价函数符合这个要求，这样，单个训练样本的二次代价函数就可以写作：

$$C = \frac {1}{2} \left \| y - a_L  \right |\ ^2 = \frac{1}{2} \sum_{j}{ }(y_j - a_j^L)^2, \qquad \qquad \qquad (27) $$

这是个关于输出激活值的函数。当然，这个代价函数也依赖于输出y，或许你会琢磨，为什么不同时把它当做y的函数呢？这是因为训练样本x是固定的，输出值y也是固定的参数。对偏移量和权重做任何改变都不会改变它们。它们不来自于神经网络学习的结果。所以，仅仅把上述函数看做$$a_L$$的函数，而把y当做是函数中的一个参数就可以了。

## 哈达玛积(点积)，$$s \odot t$$

反向传播算法基于一个通用的线性代数运算---向量乘以矩阵，就像向量加法那样。 不过没矩阵加法那么常用。具体来讲，加入s和t是两个维度相同的向量。 我们使用$$s \odot t$$记做两个向量的点积。$$s \odot t$$的元素化写法为$$s \odot t = s_jt_j$$.下面是一个例子：
$$
\begin{bmatrix}
1\\ 
2
\end{bmatrix}

\odot

\begin{bmatrix}
3\\ 
4
\end{bmatrix}

= 

\begin{bmatrix}
1*3\\ 
2*4
\end{bmatrix}

=

\begin{bmatrix}
3\\ 
8
\end{bmatrix}

\qquad \qquad \qquad (28)
$$

这种点乘有时也成为哈达玛积或者舒尔积。我们将称之为哈达玛积。优秀的矩阵库通常会提供高效的哈达玛积的计算，这对于计算反向传播算法会非常有帮助。

## 反向传播算法背后的四个基本等式

反向传播算法的意图是理解如何通过改变权重和偏移量来改变成本函数。最终，这就意味着计算偏导数$$\partial C / \partial w_jk^l $$以及$$ \partial C / \partial b_j^l$$。为了计算这个结果，我们首先引入一个中间值，$$\delta_j^l$$。代表第l层第j个神经元的误差。反向传播算法会给出以及各计算误差$$\delta _j^l$$的过程。然后，他们吧$$\delta _j^l$$与$$\partial C / \partial w_{jk}^l $$ 以及 $$\partial C / \partial b_j^l $$关联起来。
为了理解误差是如何产生的，想想一下在神经网络中有一个小怪物：

![](http://neuralnetworksanddeeplearning.com/images/tikz19.png)

小怪物坐在第l层的第j个神经元上。当输入神经元来到的时候，小怪物把神经元的运算搞的乱七八糟。它对神经元加权输入做了细微的改变 $$\Delta z_j^l$$。所以，神经元的输出从$$\delta(z_j^l)$$变为了$$\delat (z_j^l +\Delta z_j^l)$$。 这个改变通过前向传播算法往神经网络的后续层级传播，最终造成了成本函数也发生了改变$$\frac {\Delta C}{\Delta z_j^l} \Delta z_j^l $$.

假如这个小怪物是个好的小怪物，它帮助你改善了代价函数，例如，它帮助你找到了一个$$\Delta z_j^l$$使得代价函数变小了。假如$$frac {\partial C}{\partial z_j^l}$$的值非常大(无论是正值还是负值)，那么小怪物通过选择与$$\frac {\partial C}{\partial z_j^l}$$符号相反的$$\Delta z_j^l$$，减少了代价函数一大块。相反，如果$$\frac {\partial C}{\partial z_j^l}$$接近于0，就意味着小怪物没办法通过扰动加权输入$$z_j^l$$改善成本函数。这样，小怪物就可以说神经元已经十分趋近最优化了。所以，基于经验判断，$$\frac{\partial C}{\partial z_j^l}$$就是神经元误差的度量值。
  基于这个故事，我们把l层神经元j的误差$$\sigma_j^l$$以下面的公式来定义：
$$
\sigma_j^l ≡ \frac{\partial C}{\partial z_j^l} \qquad \qquad \qquad (29)
$$
习惯上，我们用$$\sigma^l$$来标记l层相关误差的向量。反向传播算法提供了一个方法，计算每一层的误差$$\sigma^l$$,然后把这些误差与实际参数($$\frac{\partial C}{\partial w_{jk}^l}$$以及$$\frac{\partial C}{\partial b_j^l})的量关联起来。

你看会琢磨为什么小恶魔会改变权重输入$$z_j^l$$。可能更加直观的是，小恶魔去改变输出激活值$$a_j^l$$，所以，我们使用$$\frac{\partial C}{\partial a_j^l}$$作为误差的度量。实际上，如果你这么做，结果会和下面讨论的内容十分相似。但是事实证明，这样的话，讲解反向传播算法在代数上就会变得有些复杂了。所以，我们仍然使用$$ \sigma_j^l ≡ \frac{\partial C}{\partial z_j^l} \qquad \qquad \qquad $$来度量误差。








