
<!--将该代码放入博客模板的head中即可-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
  inlineMath: [['$','$'], ['\\(','\\)']],
  processEscapes: true
  }
});
</script>
<!--latex数学显示公式-->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>



## sigmoid神经元


学习算法听起来有些糟糕。不过，如果我们只是弄清楚如何在一个神经网络中使用这些算法呢？假如我们打算学习使用一个perceptron网络来解决一些问题。比如，神经网络的输入的数据是真实的黑白像素数据，来自于扫描过的手写数字图片。我们让网络能够自己学习其中的权重和偏移量，然后输出结果是能够正确地对输入的数字像素进行分类。为了弄明白网络是如何学习的，我们改变网络中的权重(偏移量也可以)一点点，我们期望的结果是，网络的输出会因此也发生一个对应的小变化。这使得学习成为可能。接下来我们会阐述这一点。下面的图示可以解释我们的需要是什么(显然这个简单的网络对于解决手写数字识别来说是不够的)。

![tikz8](http://neuralnetworksanddeeplearning.com/images/tikz8.png)

假设对权重(或偏移量)做一个小的改变，则最终的输出的结果也只是发生一个小的变化，若假设成立，我们就可以利用这个机制，通过不断改变权重和偏移量来调整网络，网络将会按照我们期望的方式运行。举个例子，加入网络误将数字9误认为8，我们就可以通过微调权重和偏移量来让分类的结果更倾向是9。如果我们重复这一过程，反复调整权重和偏移量，我们将得到更准确的输出。网络就是这么学习的。

问题是，如果我们的网络包含了perceptrons，以上的机制就不灵了。实际上，无论是权重或者偏移量的微小调整，都可能导致最终输出结果的整个翻转，比如从0变为1.这个翻转将会使网络的其它部分发生完全的变化，整个过程会很复杂。尽管这个网络识别9的准确性提高了，识别其它数字的行为很可能被完全改变了，非常难以预测和控制。我们期望是通过权重或者偏移量的微小改变来让网络更接近于我们期望的方式工作，就变得十分困难了。或许会有其它更聪明的方法来绕过这个问题。但是如何让perceptrons完成学习并不是那么显而易见。
  我们可以通过引入一个新的人工神经元来克服这个问题---sigmoid神经元。他和perceptrons是类似的，不过做了优化调整，微小的权重或者偏移量的变化只会使输出产生一个小的改变。这是sigmoid神经元能够学习的一个关键特性。
  我来介绍一下sigmoid神经元，我们用描述perceptrons相同的方法来描述它。

![tikz9](http://neuralnetworksanddeeplearning.com/images/tikz9.png)

和perceptrons一样，sigmoid神经元有输入$x_1$，$x_2$，...。 不过，这些输入并不一定是0或者1，其实可以是0和1之前的任意数字，比如0.638.这对于sigmoid神经元来说也是有效的输入。和perceptrons相同，sigmoid神经元的每一个输入都有对应的权重,$w_1$,$w_2$,...，以及总体的偏移量,b.只不过输出的结果不是0或1. 而是σ(w⋅x+b)。这里σ叫做sigmoid函数，特是这么定义的：

![等式3](img/等式3.png)

![](http://latex.codecogs.com/gif.latex?\\qquad \qquad \sigma (z) \equiv \frac{1}{1+e^{-z}} \qquad \qquad (3) ）

为了更详细一些说明这一点，如果sigmoid神经元的输入是$x_1$,$x_2$,...，权重是$w_1$,$w_2$,...,偏移量是b，那么函数为：

![等式4](img/等式4.png)


![](http://latex.codecogs.com/gif.latex?\\qquad \qquad \frac{1}{1+exp(-\sum_{j}^{ }w_jx_j-b)}    \qquad \qquad      (4))

初看起来，sigmoid神经元和perceptron神经元差异非常大。如果不熟悉的话，sigmoid函数的代数表述方式看上去有些难以理解。事实上，两者之间有很多相似之处。sigmoid函数的代数方程式其实更多是技术细节，这并不会影响你理解它。
  为了理解与perceptrons模型的相似性，加入z≡w⋅x+b是一个大的正数，那么e−z≈0，所以σ(z)≈1。同样如果z≡w⋅x+b是一个很大的正数，sigmoid函数的输出为1，这点与perceptron相同。相反，如果如果z≡w⋅x+b是一个很大的负数，
![](http://latex.codecogs.com/gif.latex?\\ e^{-z} \rightarrow \infty ) ，所以 σ(z)≈0。所以，当z≡w⋅x+b是一个很大的负数，sigmoid神经元的也与perceptron相同。仅仅当w.x+b是一个之间的数值，才与perceptron模型不同。


σ的代数方程式是什么样的呢？我们该怎么理解？事实上，σ的确切代数方程式是什么并没有那么重要。重要的是这个函数的形状：

![sigmoide函数](img/sigmoid函数曲线.png)

它是阶梯函数的平滑版本：

![step函数](img/step函数曲线.png)

如果σ事实上是一个阶梯函数，那么sigmoid函数就会是一个perceptrons，因为输出的结果要么是0要么是1，取决于w⋅x+b是正数还是负数。通过使用σ函数，我们就得到了一个平滑的perceptrons。事实上，正是σ函数的平滑特性才至关重要，实际的细节公式反而不重要。正是由于σ函数平滑的特性，一个微小的权重上的变化$\Delta w_j $或者偏移量的变化$\Delta b $会导致神经元的输出$\Delta output$的微小变化。事实上，在微积分中，$\Delta output$趋向于以下函数：


xxxx[等式](5)
$\Delta output \approx \sum_{j}^{ }\frac{\partial output}{\partial w_j}\Delta w_j + \frac{\partial output}{\partial b}\Delta b $


![](http://latex.codecogs.com/gif.latex?\\Delta output \approx \sum_{j}^{ }\frac{\partial output}{\partial w_j}\Delta w_j + \frac{\partial output}{\partial b}\Delta b)

这里sum是对所有的权重 $w_j$ 求和，∂output/∂$w_j$以及 ∂output/∂b是针对$w_j$和b分别部分求导。如果你对这个部分求导不太熟悉，也不必恐慌。尽管上面的表达式看起来有些复杂，包含多个部分倒数，它想表达的内容却很简单(绝对是一个好消息)：Δoutput对于Δ$w_j$和Δb来说是一个线性函数。正是受益于这个特征，通过选择小的权重和偏移量的改变来使输出往期望的方向上发生改变，便成为可能。所以，sigmoid函数不仅仅perceptrons诸多类似的特质，它也让通过对权重和偏移量做微小改变来使输出结果朝向预期的方向发生变化成为可能。
 
如果σ的形状很重要，而不是它确切的表达式，为什么还在图3中直接这个式子呢？稍后，这本书将不时将神经元的输出f(w⋅x+b)
用另外一个激活函数 f(⋅)？？？代替。当使用不同激活函数时，主要的变化在于图5中部分求导的值会发生变化。事实证明，当我们计算这些偏导数时，使用σ会简化代数式，因为对指数求导时会产生有趣的特性。不管怎样，σ在神经网络中的使用很广泛，也是我们这本书中使用最多的激活函数。
  我们该如何理解sigmoid函数的输出呢？很明显，它和percptrons的一个很大的区别是，其输出的结果并不仅仅是0或者1.实际上是0和1之间的任一实数，诸如0.173...和0.689，这些都是合法的输出。这会很有用，例如，如果我们想用输出的结果来代表神经网络的输入---图片的像素的强度。但有时，它也可能带来困扰。比如，我们想要的网络通过输出的结果来告诉我们输入的图片是否是9，如果输出结果是0或者1，会更加简单一些，像perceptron那样。实际应用中，我们可以设置一个约定来解决这个问题。例如，如果输入的结果大于0.5则意味着是9，否则，就不是9.如果用到类似的约定，我会明确指出来，所以不会对你产生困惑。

### 练习

* sigmoid神经元模拟perceptrons，第一部分如果我们把perceptron网络中所有的权重和偏移都乘以一个正的常数c, c>0.该网络的行为不会发生任何变化。
* sigmoid神经元模拟perceptrons，第二部分
  假如，我们有一个与上个问题同样设定的perceptron网络，假定，整体的输入已经选好了，不需要具体的输入值，只要已经选定下来了就好。假如对于网络中的任一perceptron的输入x来说，w⋅x+b≠0，现在把所有的perceptron用sigmoid神经元代替，然后将每一个权重和偏移量都同样乘以一个正值的常数C。当c→∞时，sigmoid神经网络的行为和perceptron的行为就完全一致。但当w⋅x+b=0时，却不成立。

## 神经网络的结构

这一节我们讲介绍一个神经网络，它能够很好完成手写数字的分类工作。作为准备，我们先介绍几个网络组成部分的术语。假如我们有如下的网络：

![网络结构](http://neuralnetworksanddeeplearning.com/images/tikz10.png)

如前所述，最左侧的一层我们称之为输入层，其中的神经元称为输入神经元。最右一层为输出层，称为输出神经元，在此例中是一个单一的输出。中间的层称之为隐藏层，他们既不是输入层也不是输出层。隐藏这个词听起来有些神秘，我第一次听到这个术语，觉得它隐藏了很高深复杂的数学含义。不过，它真的只是意味着它既不是输入也不是输出。上面的网络只有一层隐藏层，但一些网络会有多层隐藏层。例如，下面网络有四层，其中两层是隐藏层。

![网络次四层结构](http://neuralnetworksanddeeplearning.com/images/tikz11.png)


有些难以理解的是，由于历史的原因，这类多层的网络有时候也被叫做多层perceptrons，简称做MLPs，尽管它完完全全由sigmoid神经元构成。在这本书中，我将不会使用MLP这个术语，因为它有些难以理解。当它出现时，我也会提醒你一下。

网络的输入和输出层是比较直观容易理解的。例如，假如我们打算来判断一张图片画的是不是数字9，一个自然而然的设计网络的方法是，把图片像素作为输入神经元。如果图片是64像素x64像素的灰阶图片，会有64*64=4096个输入神经元，每个神经元介于0和1之间。输出层只包含一个神经元，如果输出值小于0.5就意味着输入的图片不是数字9，高于0.5就是数字9。
  设计输入和输出层比较简单，然而，设计神经网络的隐藏层就是艺术了，你不可能通过几个简单的规则来概括隐藏层的设计过程。取而代之，神经网络研究员开发出了许多启发式的隐藏层设计方法，这些方法可以帮助人们让网络按照他们期望来行为。例如，其中的一些可以帮助在隐藏层基数与训练网络所消耗时间之间做权衡。在本书稍后章节中，我们会提到几个设计启发式。heuristics
   至此，一层的输出作为下一层的输入，我们一直在讨论这种神经网络，这类网络被称为种子向前神经网络。这意味着网络中没有循环---信息总是往前传递，从不倒回来。如果包含有循环，就意味着σ函数的输入取决于它的输出。这貌似没什么意义，所以我们不允许在网络中出现循环。
  不过，有些人工神经网络模型可能会出现反馈循环。这类模型称为循环神经网络。这些模型是想在神经元变得沉静之前，让它发射一段有限的时间。它的发射可以激发其它的神经元，然后这个被激发神经元也会发射一段有限的时间，这样就会导致更多的神经元开始发射，经过一段时间，我们就会得到神经元发射瀑布。这个模型中，循环不会造成问题，因为一个神经元的输出仅仅在一段时间之后影响它的输入，而不是同时。
  循环神经网络一致没有前馈神经网络这么有影响力，部分是因为循环神经网络的学习算法不够强大。不过，循环神经网络还是非常有趣的。相对于前馈神经网络，它们更加接近于人类大脑的工作机制，对于一些通过前馈神经网络解决起来极其困难的问题，循环神经网络却能够解决。这本书我们将着重介绍前馈神经网络。
  