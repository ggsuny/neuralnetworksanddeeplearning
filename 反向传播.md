# 第二章：反向传播
---

上一章，我们看到了神经网络是如何通过梯队下降算法学习权重和偏移量。不过，我们跳过了如何计算成本函数导数这部分。这确实是一块不小的缺失。本章我将介绍一个计算此类导数的快速算法，名字叫反向传播。

反向传播算法最早在1970年代就引入了，但直至1980年代，那篇由 David Rumelhart, Geoffrey Hinton, and Ronald Williams发表的著名论文之后，才名声鹊起。这篇论文讲述了几种神经网络，用到的反向传播算法要比之前所有的方法在学习上都快很多。这样就使得通过神经网络解决许多之前无解的问题成为可能。如今，反向传播是神经网络学习中的顶梁柱。
  相对于本书其它章节，这一章将会涉及到较多的数学知识。如果你对数学不感冒，你可能想跳过这一章节。毕竟，把反向传播当做一个黑盒，这样它的细节你完全可以不用关心。为什么要花时间学习呢？
  原因当然是为了更深入的理解。反向传播算法的核心是一个偏导表达式∂C/∂w，它是代价函数C对所有的权重w(或者偏移量b)进行求导。这个等式告诉我们，在改变w或b的时候，代价函数的变化有多快。表达式虽然有些复杂，也有它美丽的一面，每个元素都有自然和直观的解释。所以，反向传播算法不仅仅是一个快速的学习方法。它也可以帮助我们了解，如何通过改变权重和偏移量来影响网络整体行为这个过程的细节。所以还是值得学习一下他的细节。
 虽然如此，如果你想跳过这个章节，直接开始下一章节，也没问题。即使你把反向传播算法当做一个黑盒来理解，你在学习本书剩余部分的时候也没有任何问题。这本书稍后的部分可能会提到本章的一些结果。不过，对你应该没有障碍。
  ## 热身：用一个快速的基于矩阵的算法，来计算神经网络的输出
---
探讨反向传播算法之前，我们先热热身，先介绍一个快速的基于矩阵的方法，用它来计算神经网络的输出。实际上在上一张快结束的时候，我们已经看到这个算法的大概了，现在我们了解一下它的细节。这对于习惯反向传播算法中的各种符号是一个不错的方法。
  