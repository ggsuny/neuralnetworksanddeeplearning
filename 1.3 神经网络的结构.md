"plugins": [
    "katex"
]


## 神经网络的结构

这一节我们讲介绍一个神经网络，它能够很好完成手写数字的分类工作。作为准备，我们先介绍几个网络组成部分的术语。假如我们有如下的网络：

![网络结构](http://neuralnetworksanddeeplearning.com/images/tikz10.png)

如前所述，最左侧的一层我们称之为输入层，其中的神经元称为输入神经元。最右一层为输出层，称为输出神经元，在此例中是一个单一的输出。中间的层称之为隐藏层，他们既不是输入层也不是输出层。隐藏这个词听起来有些神秘，我第一次听到这个术语，觉得它隐藏了很高深复杂的数学含义。不过，它真的只是意味着它既不是输入也不是输出。上面的网络只有一层隐藏层，但一些网络会有多层隐藏层。例如，下面网络有四层，其中两层是隐藏层。

![网络次四层结构](http://neuralnetworksanddeeplearning.com/images/tikz11.png)


有些难以理解的是，由于历史的原因，这类多层的网络有时候也被叫做多层perceptrons，简称做MLPs，尽管它完完全全由sigmoid神经元构成。在这本书中，我将不会使用MLP这个术语，因为它有些难以理解。当它出现时，我也会提醒你一下。

网络的输入和输出层是比较直观容易理解的。例如，假如我们打算来判断一张图片画的是不是数字9，一个自然而然的设计网络的方法是，把图片像素作为输入神经元。如果图片是64像素x64像素的灰阶图片，会有$$64 x 64=4096$$个输入神经元，每个神经元介于0和1之间。输出层只包含一个神经元，如果输出值小于0.5就意味着输入的图片不是数字9，高于0.5就是数字9。
  设计输入和输出层比较简单，然而，设计神经网络的隐藏层就是艺术了，你不可能通过几个简单的规则来概括隐藏层的设计过程。取而代之，神经网络研究员开发出了许多启发式的隐藏层设计方法，这些方法可以帮助人们让网络按照他们期望来行为。例如，其中的一些可以帮助在隐藏层基数与训练网络所消耗时间之间做权衡。在本书稍后章节中，我们会提到几个设计启发式。heuristics
   至此，一层的输出作为下一层的输入，我们一直在讨论这种神经网络，这类网络被称为种子向前神经网络。这意味着网络中没有循环---信息总是往前传递，从不倒回来。如果包含有循环，就意味着σ函数的输入取决于它的输出。这貌似没什么意义，所以我们不允许在网络中出现循环。
  不过，有些人工神经网络模型可能会出现反馈循环。这类模型称为[循环神经网络](http://en.wikipedia.org/wiki/Recurrent_neural_network)。这些模型是想在神经元变得沉静之前，让它发射一段有限的时间。它的发射可以激发其它的神经元，然后这个被激发神经元也会发射一段有限的时间，这样就会导致更多的神经元开始发射，经过一段时间，我们就会得到神经元发射瀑布。这个模型中，循环不会造成问题，因为一个神经元的输出仅仅在一段时间之后影响它的输入，而不是同时。
  循环神经网络一致没有前馈神经网络这么有影响力，部分是因为循环神经网络的学习算法不够强大。不过，循环神经网络还是非常有趣的。相对于前馈神经网络，它们更加接近于人类大脑的工作机制，对于一些通过前馈神经网络解决起来极其困难的问题，循环神经网络却能够解决。这本书我们将着重介绍前馈神经网络。
  
